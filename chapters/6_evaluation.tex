\section{Evaluation}\label{sec:evaluation}
We have already looked at other results in the literature in section \ref{sec:practical-results-lit} which gave a decent comparison between some of the algorithms. However, we did not get a lot of knowledge about the datasets, especially the structure of the preference list and distribution of those preferences. Therefore we will conduct an experiment, by benchmarking the implemented algorithms with both real and artificial datasets of varying sizes and preference structures.

\subsection{Research Questions}\label{sec:research-q}
Table \ref{tab:algorithm-comparison} already compared and summarized the algorithms from a theoretical perspective, however some of the results would benefit from further quantification and clarity. For instance, we know that the Max-Popular algorithm, as described in section \ref{algo-max-pop}, does not always produce a matching. It would be beneficial to empirically investigate if certain inputs cause this problem. Therefore we will formalize some open questions as research questions, that this experiment hopes to answer:
\begin{enumerate}
    \item What is the effect of different preference distributions on the matchings produced by the different algorithms.
    \item How likely is it for the Max-Popular algorithm to not produce a matching using different datasets?
    \item How does the Modified-Popular algorithm from section \ref{impl:mod-max-pop} compare in terms of popularity to the other algorithms.
    \item What is the cost of giving up on strategy proofness in terms of rank, i.e. how well does RSD perform compared to other mechanisms?
    \item Is Popularity a meaningful metric for the student-seminar use case?
    \item What is the impact of short-preference lists on the quality and existence of matchings?
\end{enumerate}

\subsection{Sample Data}
Due to the fact that real data for seminar registrations is mostly kept private, a majority of the benchmark will rely on synthetic data. However, there are a few publicly available datasets available that will be used for evaluation and also for better understanding real-world preference distributions.

\subsubsection{Using graph generators}
Using graph generators to synthetically create instances to the seminar-student matching problem turned out to be a big challenge. Essentially, we would need a graph generator for bipartite, weighted graphs, that also considers the seminar capacity. One option would be generating as many nodes as the total capacity of all seminars, but that could result in duplicate edges or i.e. preferences. Therefore we will use a custom, domain specific mechanism for generating instances.

\subsubsection{Random Uniform Preference Lists}
Probably the simplest way to synthetically generate data, was picking a size for the instance and then generating seminars and students based on a uniform random generator. The preference lists are created by shuffling the list of all seminars for each student. Additionally, we can simply shorten each preference list by a random factor as well for measuring the effect of incomplete-preference lists.

\subsubsection{PrefLib Datasets}
Preflib.org is an open collection of more than 3000 community-contributed datasets of preference data for different domains \cite{PrefLib}. Fortunately, there exist two datasets of students' course preferences at the polish AGU University \cite{preflib-dataset}. The two datasets contain strict and complete preference lists for each students with 9 courses and 146 students or 7 courses and 153 students respectively. Unfortunately there are no capacities given, however those will be computed synthetically using a uniform distribution. An interesting characteristic of those dataset is that in each dataset, all students rank the same seminar with their first preference. I decided to those seminars from the dataset, because I wouldn't assume such a preference distribution in the real world.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{assets/plots/preflib1-distribution.png}
      \caption{Preference Distribution first dataset.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{assets/plots/preflib2-distribution.png}
      \caption{Preference Distribution second dataset.}
    \end{subfigure}
    \caption{Preference Distributions for first-ranked seminars for both PrefLib datasets}
    \label{fig:preflib-distribution}
  \end{figure}

Figure \ref{fig:preflib-distribution} shows the preference distributions for the student's first choice in both datasets after removing the always first-ranked seminar. We can see that in both datasets, students clearly strongly prefer two seminars. However in the first dataset, there are 3 more seminars that are also decently popular, while in the second dataset the majority of students really prefers the two dominant seminars. Generally, those datasets indicate that the preference structure is perhaps not likely to be uniformly distributed.

\subsubsection{Random Zipf-Distributed Preference Lists}
To better simulate real-world preference structures than by using a uniform distribution, a power-law distribution can be applied. According to Zipf's law the frequency of a word in a large sample, is proportional to it's position in a frequency table. This law can also be applied for creating synthetic seminar distribution. Using this type of distribution with some additional randomization yields the following preference list structure when seminar and student counts are similar to the first PrefLib dataset:

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{assets/plots/zipfian1-distribution.png}
      \caption{Preference Distribution first sample.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
      \includegraphics[width=\linewidth]{assets/plots/zipfian2-distribution.png}
      \caption{Preference Distribution second sample.}
    \end{subfigure}
    \caption{Zipfian Preference Distribution for first-ranked seminars}
    \label{fig:zipfian-distribution}
  \end{figure}

To create the dataset, for each student a seminar is randomly drawn using the Zipfian distribution and added to the his preference list until the desired preference-list length has been reached. Comparing Figure \ref{fig:zipfian-distribution} to Figure \ref{fig:preflib-distribution} shows somewhat similar results, which should make this generator relevant for the benchmark. 

\subsubsection{Carnegie-Mellon Datasets}

\subsection{Methodology}
The goal of this experiment is quantifying some of the optimality criteria defined in section \ref{sec:optimality} to compare the selection of algorithms empirically and find answers to the research questions proposed in section \ref{sec:research-q}. Due to the fact that real-world data for student enrollments is not widely available, it is important to note that the results of this experiment will be biased by the selection of data available. However, by using the available real datasets and a few different synthetic datasets it should be possible to find answers to the proposed research questions and to better understand the differences of different approaches.

\subsubsection{Benchmark Setup}
The benchmark is performed by a Kotlin program that generates test data and then executes each of the algorithms with the same input. The program then collects the results and computes statistics on each matching, which are saved into a file. When using synthetic data, the program will generate 50 different instances to account for the randomness. The following type of instances will be used:
\begin{enumerate}
  \item PrefLib Datasets
  \item Medium Sized Random Dataset ($\sim$100 students)
  \begin{enumerate}
    \item with uniform preference distribution
    \item with Zipfian preference distribution
  \end{enumerate}
  \item Large Sized Random Dataset ($\sim$1000 students)
  \begin{enumerate}
    \item with uniform preference distribution
    \item with Zipfian preference distribution
  \end{enumerate}
\end{enumerate}

\subsubsection{Metrics}
We already know that all of the algorithms under considerations produce pareto-optimal matchings, however it will be necessary to measure popularity. If the Popular-CHA algorithm finds a matching, we know that it's the popular matching, however if the algorithm cannot find a matching it could be possible that one of the other algorithms produces such a matching. In general, given the definitions for Popularity that we have used before, it is not possible to check if a given matching is popular without comparing it to all other matchings. However this is infeasable due to the exponential runtime complexity of that comparison, which is why we will just compare the matchings produced by the five algorithms used in the evaluation for Popularity. Given two matchings $m, m' \in \mathcal{M}$, we say that $m$ is more popular than $m'$ if the number of students prefering $m$ is greater than the number of students prefering $m'$. 
In addition to comparing Popularity, the following other metrics will be used:
\begin{itemize}
  \item \textbf{Profile}: The profile of the matching as defined in section \ref{sec:profile} given as an array.
  \item \textbf{Average Rank \& Standard Deviation}: The average and standard deviation of the matched students' ranks. A matched student's rank corresponds to the position of his match on his preference list.
  \item \textbf{Worst Rank}: In conjunction with the previous metric, we will also look at the worst rank that exists in a matching.
  \item \textbf{Cardinality}: The size of a matching.
  \item \textbf{Unassigned-Count}: The number of unassigned students in a matching.
  \item \textbf{Runtime}: The runtime of the algorithm in milliseconds.
  \item \textbf{Existence}: This metric is only interesting for the Popular-CHA algorithm and will indicate if the algorithm was able to compute a matching for a given instance.
\end{itemize}

These metrics should be a sufficient selection to quantify a matching's optimality and therefore should make it possible to answer the research questions from section \ref{sec:research-q}.

\subsection{Results}

\subsection{Conclusion}