\section{Comparison of mechanisms}
In the previous chapter we have studied several different algorithmic approaches, which guarantee different optimality criteria and rely on different mechanisms. To better evaluate the algorithms against the optimality criteria we defined in chapter \ref{sec:optimality} we will now summarize and compare the properties of the algorithms. Afterwards we will look at some practical results, that were obtained by a benchmark of matching mechanisms with one- and two-sided preferences by Diebold and Bichler.\cite{DieboldBenchmark} These results will provide a guideline in picking algorithms for implementation and will be described in the next chapter.

\subsection{Theoretical results}
Drawing back to the list of desirable properties defined in section \ref{criteria-application}, let us now recap and compare the aforementioned algorithms to evaluate which one could be applicable for the problem of matching students to seminars. Unfortunately none of the algorithms guarantee all of the optimality critera at the same time, which makes the choice of an algorithm unclear. Table \ref{tab:algorithm-comparison} gives an overview of the presented algorithms and their properties. Each of the algorithms is listed in the same order that they were presented, and for each optimality criteria, a yes/no encoding is used to make a statement about which properties an algorithm guarantees. It is important to note here that a "no" in a column does not strictly mean that the given optimality criteria cannot be fulfilled by the algorithm, but rather that the algorithm does not guarantee it. For instance, a matching computed with the greedy algorithm can be of maximum cardinality or be popular. Only the results for strategy-proofness are a strict yes or no, since fulfilling strategy-proofness does not depend on the instance of the problem, but only of the mechanism being used. 

\begin{table}[h!]
    \begin{tabular}{lllll}
    \hline
                        & Greedy & Max Pareto   & Assignment & Popular           \\ \hline
    Maximum Cardinality & no     & yes          & yes        & yes               \\
    Pareto-Optimal      & yes    & yes          & yes        & yes               \\
    Popular             & no     & no           & no         & yes               \\
    Rank Maximal        & no     & no           & yes        & no                \\
    Always Exists       & yes    & yes          & yes        & no                \\
    Strategy Proof      & yes    & no           & no         & yes               \\ \hline
    Time Complexity     & $\mathcal{O}(n)$   & $\mathcal{O}(\sqrt{n}m)$ & $\approx\mathcal{O}(n^3)$    & $\mathcal{O}(\sqrt{C}n_1 + m)$ \\ \hline
    \end{tabular}
    \caption{Comparison of different algorithmic approaches}
    \label{tab:algorithm-comparison}
\end{table}

To summarize the results, we can see that all of the algorithms guarantee pareto-optimality, however only the Popular-CHA algorithm guarantees popularity. At the same time, only the greedy approach and Popular-CHA also guarantee strategy-proofness, which makes Popular-CHA particularly interesting for the student-seminar problem. 

\subsubsection{Strategy-proofness and maximum cardinality}
One interesting observation is that fulfilling maximum cardinality comes at the cost of either not being strategy-proof, or not guaranteeing that a matching exists at all. Indeed, only the greedy and Popular-CHA algorithm guarantee strategy-proofness. However, ensuring strategy-proofness and maximum cardinality at the same time comes at the cost of not always finding a matching. If we look back at the algorithm Popular CHA, we remember that a maximum cardinality matching $M'$ is computed on the reduced graph $G'$. We saw that a maximum popular matching does not exist, iff the matching is not agent-complete, meaning that one of the agents is matched to their last-resort house. While this mechanism ensures strategy-proofness, it is also not always possible to find such a maximum cardinality matching using the Popular CHA algorithm. Therefore, it remains an open question whether or not a mechanism exists that both is strategy-proof and produces maximum-cardinality matchings.

\subsubsection{Max-PaCHA and the assignment problem}
Another important thing to notice is the similarity of the properties between the Max-PaCHA and assignment problem algorithm. Except for the fact that the assignment algorithm guarantees rank maximality, the two algorithms produce matchings with very similar characteristics, which then begs the question why one should use the Max-PaCHA algorithm. But looking at the runtime complexity of the algorithms, we see that, while both algorithms run in polynomial time, the assignment problem takes longer to be solved.

\subsection{Practical results}
In the past, the technical university of Munich (TUM) assigned students to courses via a first-come first-served mechanism, which resulted in complaints from both students and lecturers.\cite{tum-matching} After Diebold et al. \cite{Diebold2014} at TUM investigated different mechanisms for this matching problem, the university switched to a stable matching mechanism, which is based on Gale \& Shapley's deferred acceptance mechanism. It has to be noted here, that the university uses two-sided preferences for their system, which makes their results less applicable for this thesis, however Diebold et al. have also published an extensive benchmark on matching mechanisms with both one- and two-sided preferences.\cite{DieboldBenchmark} They used real course registration data from TUM, grouped into 28 datasets for investigating properties, including size, rank and popularity, of matchings produced by several mechanisms. 

\subsubsection{Dataset}
The data with one-sided preferences is comprised of 9 datasets from the official tutorial registration at TUM in the period between October 2012 and October 2015. All of those datasets contain incomplete preference lists and all but two of them contain ties. Each dataset contains between 136 and 1035 students and between 5 and 51 courses with total capacities ranging from 130 to 1282. For the dataset with 51 courses, the authors provide a histogram of the length of student's preference lists, showing that a majority of students gave preferences for between 10 and 15 tutorials.\cite{DieboldBenchmark} Unfortunately the rest of the data is kept private and therefore it's not possible to get information on the distribution of the indiviudal preferences lists, or in other words the popularity of each seminar from the students' perspective.

\subsubsection{Algorithms used}
Diebold et al. used most of the mechanism described in section \ref{chapter:algorithms}, with some small differences. We will primarily compare their results for the following algorithms:
\begin{enumerate}
    \item RSD - Random Serial Dictatorship, see \ref{algo-rsd}
    \item MPO CHA - Max-Pareto-Optimal CHA, see \ref{algo-max-po}
    \item ProB CHAT - Profile-based optimal algorithm with ties. Produces the same matching as the assignment problem algorithm, see \ref{algo:assignment}
    \item Pop CHAT - Max Popular Algorithm with ties. An extension of Pop-CHA, see \ref{algo-max-pop}
\end{enumerate}

\subsubsection{Methodology}
The authors implemented all matching mechanisms in Python 3.4.2 and used randomization to break ties. Additionally, every algorithm was run 100 times and the average of the metrics were obtained for each mechanism. Additionally some of the following metrics were used for the one-sided case:
\begin{enumerate}
    \item \textbf{Size}: Number of students being matched.
    \item \textbf{Average Rank}: A rank for a student is the position of the student's assigned course on his preference list. The average is taken over all students for this metric.
    \item \textbf{Popularity}: The mechanisms are compared against each other to check which mechanism produces more popular matchings. 
    \item Average \textbf{AUPCR} (Area under the Profile Curve Ratio): This metric was introduced to compare rank profiles of different matchings. In the words of the authors: "The AUPCR up to a specific rank describes the probability that a matching mechanism will rank a randomly chosen student higher than his n-th preference."\cite{DieboldBenchmark}
\end{enumerate}

\subsubsection{Results}
A summary of the benchmark is provided in Table \ref{tab:diebold-benchmark}. Unsurprisingly the Pop CHAT mechanism produces more popular matchings than all other mechanisms, but it's also interesting to note that ProB CHAT produces more popular matchings than all mechanisms but Pop CHAT. Besides that we can see that all mechanisms achieve an average size of atleast 97.4\%, while MPO CHA and ProB CHAT unsurprisingly find perfect matchings.

\begin{table}[h!]
    \centering 
    \begin{tabular}{|l|l|l|l|l|}
        \hline
        Metric & RSD & MPO CHA & ProB CHAT & Pop CHAT \\ \hline
        Average AUPCR & 94.95\% & 96.77\% & \cellcolor[HTML]{9AFF99}97.83\% & 97.31\% \\ \hline
        More Popular & 20\% & 0\% & 80\% & \cellcolor[HTML]{9AFF99}100\% \\ \hline
        Average rank & 1.41 & 1.51 & \cellcolor[HTML]{9AFF99}1.26 & 1.33 \\ \hline
        Average size & 97.48\% & \cellcolor[HTML]{9AFF99}100\% & \cellcolor[HTML]{9AFF99}100\% & 99.78\% \\ \hline
        Max Runtime & \cellcolor[HTML]{9AFF99}0.014s & 0.522s & \cellcolor[HTML]{FFCCC9}33.852s & 2.458s \\ \hline
        \end{tabular}
    \caption{Summary of one-sided matching mechanisms from Diebold et al. \cite{DieboldBenchmark}}
    \label{tab:diebold-benchmark}
\end{table}

Looking at the profile and rank, we can see that ProB CHAT performs best, which is not a surprise, but at the same time Pop CHAT got very close in terms of average AUPCR and average rank. However it's important to note that Pop CHAT didn't find a matching for one of the 9 datasets. At first glance it would seem that ProB CHAT always produces really good results in terms of the metrics used, however we also need to note that it's maximum runtime in the benchmark was 33.852 seconds for the largest dataset with 915 students and 51 courses with a total capacity of 1080. In contrast, the RSD mechanism still produces acceptable results at runtimes of a fraction of a second, while being strategy-proof.

\subsubsection{Learnings}
In this benchmark, we have seen that the ProB CHAT mechanism unsurprisingly produces some of the best results, while also being limited by its runtime which is worse among all mechanisms. Additionally, we have seen that Pop CHAT also produces good matchings at a more acceptable runtime. One interesting observation is that Pop CHAT produces better results than MPO CHA, if it can find a matching. While these results confirm some of the theoretical observations, it will be interesting to get more insights with different data being used. In this benchmark, most of the data contained ties and we didn't get any insights on the distribution of preferences. 